#Evals
- _One-sentence summary_: make tools that can actually check whether a model has a certain capability / misalignment mode. We default to low-n sampling of a vast latent space but aim to do better.
- _Theory of change:_ most models have a capabilities overhang when first trained and released; we should keep a close eye on what capabilities are acquired when so that frontier model developers are better informed on what security measures are already necessary (and hopefully they extrapolate and eventually panic).
- Grouping together [ARC Evals](https://evals.alignment.org/)_,_ [Deepmind](https://arxiv.org/abs/2305.15324), [Cavendish](https://cavendishlabs.org/publications/), [situational awareness](https://arxiv.org/abs/2309.00667) crew, [Evans and Ward](https://www.matsprogram.org/evals), [Apollo](https://www.apolloresearch.ai/blog/understanding-da-and-sd). See also [Model Psychology](https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp); neuroscience : psychology :: interpretability : model psychology. See also [alignment evaluations](https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations). See also [capability](https://arxiv.org/abs/2304.11158) [prediction](https://arxiv.org/abs/2202.07785) and the hundreds of trolls doing _ahem_ [decentralised](https://www.youtube.com/watch?v=g7YJIpkk7KM) [evals](https://twitter.com/jbrowder1/status/1652387444904583169).
- _Some names:_ Mary Phuong, Toby Shevlane, Beth Barnes, Holden Karnofsky, Lawrence Chan, Owain Evans, Francis Rhys Ward, Apollo, Palisade, OAI Preparedness
- _Estimated # FTEs:_ 13 (ARC), ~50 elsewhere
- _Some outputs in 2023_: [AI AI research](https://arxiv.org/abs/2310.03302), [autonomy](https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf), [Do the Rewards Justify the Means?](https://arxiv.org/abs/2304.03279), [Stubbornness](https://arxiv.org/abs/2304.12280). [Naming](https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers) the thing that GPTs have become was useful. [Tag](https://www.lesswrong.com/tag/ai-evaluations).
- _Critiques:_ [_Hubinger_](https://www.lesswrong.com/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations)_,_ [_Hubinger_](https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations)_,_ [_Shovelain & Mckernon_](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals) 
- _Funded by:_ Various. 
- _Trustworthy command, closure, opsec, common good, alignment mindset: ?_
- _Resources:_ ~~$20,000,000 not counting the new government efforts

[[ARC Evals]]
[[DeepMind]]
[[Cavendish]]
[[the situational awareness crew]]
[[Evans and Ward]]
[[Apollo]]