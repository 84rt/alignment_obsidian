#Evals 
- _One-sentence summary_: let’s attack current models and see what they do / deliberately induce bad things on current frontier models to test out our theories / methods. See also [gain of function](https://forum.effectivealtruism.org/posts/yCx3kCReJtucpdd33/the-current-alignment-plan-and-how-we-might-improve-it-or#Gain_of_function) experiments (producing demos and toy models of misalignment. See also “Models providing Critiques”. See also: threat modelling ([Model Organisms](https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1), [Powerseeking](https://arxiv.org/abs/2304.06528), [Apollo](https://www.apolloresearch.ai/blog/understanding-da-and-sd)); [steganography](https://arxiv.org/abs/2310.18512); [part](https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=phFqC2EdDALCFwr56) of OpenAI’s superalignment schedule; [Trojans](https://arxiv.org/abs/2302.10894) ([CAIS](https://trojandetection.ai/tracks)); [Latent Adversarial Training](https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training#comments) is an unusual example. 
- _Some names:_ Stephen Casper, Lauro Langosco, Jacob Steinhardt, Nina Rimsky, Jeffrey Ladish/Palisade, Ethan Perez, Geoffrey Irving, [ARC Evals](https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker), Apollo, Dylan Hadfield-Menell/[AAG](https://algorithmicalignment.csail.mit.edu/team/)
- _Estimated # FTEs:_ ?
- _Some outputs in 2023_: [Rimsky](https://www.lesswrong.com/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering), [Wang](https://far.ai/publication/wang2022adversarial/), [Wei](https://arxiv.org/abs/2307.02483), [Tong](https://arxiv.org/abs/2306.12105), [Casper](https://arxiv.org/abs/2306.09442), [Ladish](https://arxiv.org/abs/2311.00117), [Langosco](https://docs.google.com/document/d/1xdidMGrzHDIJ5sG2h1TKb-HiGh-hgsMY3jugnvd6Aqo/edit#heading=h.9axbdg9kpll0), [Shah](https://arxiv.org/abs/2311.03348), [Scheurer](https://arxiv.org/abs/2311.07590), [AAG](https://algorithmicalignment.csail.mit.edu/research/), 2022: [Irving](https://arxiv.org/abs/2202.03286) 
- _Critiques:_ ?
- _Funded by: Various_
- _Trustworthy command, closure, opsec, common good, alignment mindset: ?_
- _Resources:_ Large



[[Stephen Casper]]
[[Lauro Langosco]]
[[Jacob Steinhardt]]
[[Nina Rimsky]]
[[Jeffrey Ladish]]
[[Palisade]]
[[Ethan Perez]]
[[Geoffrey Irving]]
[[ARC Evals]]
[[Apollo]]
[[Dylan Hadfield-Menell]]
[[AAG]]
